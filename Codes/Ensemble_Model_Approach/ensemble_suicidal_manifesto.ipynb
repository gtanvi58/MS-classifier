{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DYprpLdymIXu"
      },
      "source": [
        "This code generates model2 of the ensemble by training (Mistral fine tuned on Suicidal dataset) on the Mass Shooter Manifestos dataset\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "liZ1K0iuvI8g"
      },
      "outputs": [],
      "source": [
        "# Upload the following files in the runtime:\n",
        "# train_dataset.csv\n",
        "# suicidal_model.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qTlIQcjVgQKo"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "!pip install --no-deps \"xformers<0.0.26\" trl peft accelerate bitsandbytes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Import the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rec0xLmht1gW"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Define the path to the uploaded zip file\n",
        "zip_file_path = './suicidal_model.zip'\n",
        "\n",
        "# Define the directory where you want to extract the contents\n",
        "extract_dir = './suicidal_model'\n",
        "\n",
        "# Create the extraction directory if it doesn't exist\n",
        "if not os.path.exists(extract_dir):\n",
        "    os.makedirs(extract_dir)\n",
        "\n",
        "# Unzip the file\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_dir)\n",
        "\n",
        "print(\"Extraction complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8dvcFpnqkrYF"
      },
      "outputs": [],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "max_seq_length = 2048\n",
        "dtype = None\n",
        "load_in_4bit = True\n",
        "\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"suicidal_model\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        "\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Import the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FHDS58Y-lAh5"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from datasets import Dataset, load_dataset\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "# Load data from CSV file\n",
        "train_dataset = pd.read_csv('./train_dataset.csv')\n",
        "val_dataset = pd.read_csv('./val_dataset.csv')\n",
        "\n",
        "X_train = train_dataset['text'].tolist()\n",
        "y_train = train_dataset['label'].tolist()\n",
        "\n",
        "X_val = val_dataset['text'].tolist()\n",
        "y_val = val_dataset['label'].tolist()\n",
        "\n",
        "train_dataset = Dataset.from_dict({\"text\": X_train, \"label\": y_train})\n",
        "val_dataset = Dataset.from_dict({\"text\": X_val, \"label\": y_val})\n",
        "\n",
        "# Move model to GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define prompt for training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hYKnH1GVlLMK"
      },
      "outputs": [],
      "source": [
        "from transformers import PreTrainedTokenizerFast\n",
        "\n",
        "prompt_template = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Output:\n",
        "{}\"\"\"\n",
        "\n",
        "EOS_TOKEN = tokenizer.eos_token\n",
        "\n",
        "texts=[]\n",
        "def formatting_prompts_func(examples):\n",
        "    instruction = \"\"\"You are an expert in identifying texts made by individuals associated with mass shootings. Your task is to analyze the provided text and determine if it exhibits characteristics typical of individuals associated with mass shootings.\n",
        "\n",
        "Chain of Thought:\n",
        "\n",
        "1. Begin by reading the provided text thoroughly, paying attention to language, tone, and underlying emotions.\n",
        "2. Evaluate the text within this context, considering whether it demonstrates signs such as feelings of depression, suicidal thoughts, racist mentality, or a fascination with violence.\n",
        "3. Based on your analysis, classify the text into one of the following categorical labels:\n",
        "   - 1: If the text demonstrates patterns or behaviors commonly associated with individuals prone to mass shootings.\n",
        "   - 0: If the text does not exhibit such patterns or behaviors.\n",
        "\"\"\"\n",
        "\n",
        "    input_text = examples[\"text\"]\n",
        "    output_label = examples[\"label\"]\n",
        "\n",
        "    s = prompt_template.format(instruction, input_text, output_label) + ' ' + EOS_TOKEN\n",
        "\n",
        "    texts.append(s)\n",
        "    return { \"text\" : s }\n",
        "\n",
        "# Assuming you have already loaded your dataset\n",
        "dataset = train_dataset.map(formatting_prompts_func)\n",
        "print(dataset)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Train the model using N-pair loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UulCY6BAlT3j"
      },
      "outputs": [],
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Define the NpairLoss class\n",
        "class NpairLoss(nn.Module):\n",
        "    def __init__(self, l2_reg=0.02):\n",
        "        super(NpairLoss, self).__init__()\n",
        "        self.l2_reg = l2_reg\n",
        "\n",
        "    def forward(self, anchor, positive, target):\n",
        "        batch_size = anchor.size(0)\n",
        "        target = target.view(target.size(0), 1)\n",
        "\n",
        "        target = (target == torch.transpose(target, 0, 1)).float()\n",
        "        target = target / torch.sum(target, dim=1, keepdim=True).float()\n",
        "\n",
        "        logit = torch.matmul(anchor, torch.transpose(positive, 0, 1))\n",
        "        loss_ce = F.cross_entropy(logit, target)\n",
        "        l2_loss = torch.sum(anchor**2) / batch_size + torch.sum(positive**2) / batch_size\n",
        "\n",
        "        loss = loss_ce + self.l2_reg*l2_loss*0.25\n",
        "        return loss\n",
        "\n",
        "# Create an instance of the NpairLoss\n",
        "npair_loss = NpairLoss()\n",
        "\n",
        "# Instantiate the trainer without specifying compute_loss argument\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=dataset,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    dataset_num_proc=2,\n",
        "    packing=False,\n",
        "    args=TrainingArguments(\n",
        "        per_device_train_batch_size=2,\n",
        "        gradient_accumulation_steps=4,\n",
        "        warmup_steps=5,\n",
        "        max_steps=60,\n",
        "        learning_rate=2e-4,\n",
        "        fp16=not torch.cuda.is_bf16_supported(),\n",
        "        bf16=torch.cuda.is_bf16_supported(),\n",
        "        logging_steps=1,\n",
        "        optim=\"adamw_8bit\",\n",
        "        weight_decay=0.01,\n",
        "        lr_scheduler_type=\"linear\",\n",
        "        seed=3407,\n",
        "        output_dir=\"outputs\",\n",
        "    )\n",
        ")\n",
        "\n",
        "# Set the custom loss function to the model\n",
        "trainer.model.compute_loss = npair_loss\n",
        "trainer_stats = trainer.train()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define the prompt for testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DLDlHp3IxNcH"
      },
      "outputs": [],
      "source": [
        "from transformers import PreTrainedTokenizerFast\n",
        "\n",
        "prompt_template = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Output:\n",
        "\"\"\"\n",
        "\n",
        "EOS_TOKEN = tokenizer.eos_token\n",
        "\n",
        "texts = []\n",
        "def formatting_prompts_func(examples):\n",
        "    instruction = \"\"\"You are an expert in identifying texts made by individuals associated with mass shootings. Your task is to analyze the provided text and determine if it exhibits characteristics typical of individuals associated with mass shootings.\n",
        "\n",
        "Chain of Thought:\n",
        "\n",
        "1. Begin by reading the provided text thoroughly, paying attention to language, tone, and underlying emotions.\n",
        "2. Evaluate the text within this context, considering whether it demonstrates signs such as feelings of depression, suicidal thoughts, racist mentality, or a fascination with violence.\n",
        "3. Based on your analysis, classify the text into one of the following categorical labels:\n",
        "   - 1: If the text demonstrates patterns or behaviors commonly associated with individuals prone to mass shootings.\n",
        "   - 0: If the text does not exhibit such patterns or behaviors.\n",
        "\"\"\"\n",
        "\n",
        "    input_text = examples[\"text\"]\n",
        "    #output_label = examples[\"label\"]\n",
        "\n",
        "    s = prompt_template.format(instruction.strip(), input_text)\n",
        "\n",
        "    texts.append(s)\n",
        "    return { \"text\" : s }\n",
        "\n",
        "# Assuming you have already loaded your dataset\n",
        "test_dataset = val_dataset.map(formatting_prompts_func)\n",
        "print(test_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Test the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UCIa2phUxNHg"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import time\n",
        "\n",
        "generated_outputs = []\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "# Iterate through the test dataset\n",
        "for idx in range(len(test_dataset['text'])):\n",
        "    # Generate input prompt\n",
        "    prompt_input = tokenizer(test_dataset['text'][idx],return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "    # Generate output\n",
        "    outputs = model.generate(**prompt_input, max_new_tokens = 64, use_cache = True).to(\"cuda\")\n",
        "    #print(outputs)\n",
        "\n",
        "    #print(tokenizer.batch_decode(outputs))\n",
        "    # Append generated output to the list\n",
        "    generated_outputs.append(tokenizer.batch_decode(outputs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FpMCsbH4xM_v"
      },
      "outputs": [],
      "source": [
        "generated_labels = [item[0].split(\"### Output:\\n\")[1].strip().split(\" \")[0] for item in generated_outputs]\n",
        "\n",
        "print(list(set(generated_labels)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Save the predicted labels and the trained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OgigzMcXxM4v"
      },
      "outputs": [],
      "source": [
        "# save generated labels\n",
        "import pickle\n",
        "\n",
        "# File path in the Colab runtime\n",
        "file_path = '/content/generated_labels2.pkl'\n",
        "\n",
        "# Save the list to a file\n",
        "with open(file_path, 'wb') as file:\n",
        "    pickle.dump(generated_labels, file)\n",
        "\n",
        "print(f\"Labels saved to {file_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gfl3RIoIlump"
      },
      "outputs": [],
      "source": [
        "model.save_pretrained(\"model_2\")\n",
        "tokenizer.save_pretrained(\"model_2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b3eNzz0Xlysm"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "\n",
        "# Zip the folder containing model and tokenizer files\n",
        "shutil.make_archive(\"model_2\", \"zip\", \"model_2\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
