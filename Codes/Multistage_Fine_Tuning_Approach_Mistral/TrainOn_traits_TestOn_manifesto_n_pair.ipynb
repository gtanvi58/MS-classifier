{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Install unsloth using a GPU"
      ],
      "metadata": {
        "id": "Ip3KTh6c1ctu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yHyvh1s1tpQB"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "!pip install --no-deps \"xformers<0.0.26\" trl peft accelerate bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "max_seq_length = 2048\n",
        "dtype = None\n",
        "load_in_4bit = True\n",
        "\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/mistral-7b-bnb-4bit\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        "\n",
        ")\n",
        "\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16,\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0,\n",
        "    bias = \"none\",\n",
        "\n",
        "    use_gradient_checkpointing = \"unsloth\",\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,\n",
        "    loftq_config = None,\n",
        ")"
      ],
      "metadata": {
        "id": "FXXquI2zIrQr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Preprocessing for White Suprimacist dataset"
      ],
      "metadata": {
        "id": "xAH2T-V_I3ES"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
        "\n",
        "# Load data from JSON files\n",
        "with open('./annotated_data-train.json', 'r') as f:\n",
        "    train_data = json.load(f)\n",
        "\n",
        "with open('./annotated_data-test.json', 'r') as f:\n",
        "    test_data = json.load(f)\n",
        "\n",
        "\n",
        "train_df = pd.DataFrame(train_data)\n",
        "test_df = pd.DataFrame(test_data)\n",
        "\n",
        "\n",
        "df = pd.concat([train_df, test_df])\n",
        "\n",
        "data = df[['text', 'label']]\n",
        "\n",
        "\n",
        "# Move model to GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "# Split data into train and validation sets\n",
        "X = list(data[\"text\"])\n",
        "y = list(data[\"label\"])\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, stratify=y)\n",
        "\n",
        "\n",
        "from datasets import Dataset, load_dataset\n",
        "train_dataset = Dataset.from_dict({\"text\": X_train, \"label\": y_train})\n",
        "val_dataset = Dataset.from_dict({\"text\": X_val, \"label\": y_val})\n",
        "\n",
        "# Mapping labels to numerical values\n",
        "label_map = {'hate': 1, 'noHate': 0}\n",
        "\n",
        "# Apply mapping to train and validation labels\n",
        "train_dataset = train_dataset.map(lambda example: {'label': label_map[example['label']]})\n",
        "val_dataset = val_dataset.map(lambda example: {'label': label_map[example['label']]})"
      ],
      "metadata": {
        "id": "-1l0Yr1YItij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(list(set(train_dataset['label'])))"
      ],
      "metadata": {
        "id": "qMqavXh2JSMz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating Prompts"
      ],
      "metadata": {
        "id": "OJHPFe1-JPr1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import PreTrainedTokenizerFast\n",
        "\n",
        "prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Output:\n",
        "{}\"\"\"\n",
        "\n",
        "EOS_TOKEN = tokenizer.eos_token\n",
        "\n",
        "texts=[]\n",
        "def formatting_prompts_func(examples):\n",
        "    instruction = \"You are an expert in identifying whether a statement contains sentiments against certain racial groups or ethnic communities. Given a statement, your task is to classify the given text to determine if it holds sentiments against racial groups or ethnic communities.\"\n",
        "    input = examples[\"text\"]\n",
        "    #print(\"input: \",input)\n",
        "    output = examples[\"label\"]\n",
        "    #print(\"output:\",output)\n",
        "\n",
        "    s=prompt.format(instruction, input, output)+' '+EOS_TOKEN\n",
        "    #print(s)\n",
        "    texts.append(s)\n",
        "    return { \"text\" : s }\n",
        "\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "\n",
        "dataset=train_dataset\n",
        "dataset = dataset.map(formatting_prompts_func)\n",
        "print(dataset)\n",
        "\n"
      ],
      "metadata": {
        "id": "rA29BbKFJWTr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training Model on N-pair loss"
      ],
      "metadata": {
        "id": "YdlyzFQzJhZx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Define the NpairLoss class\n",
        "class NpairLoss(nn.Module):\n",
        "    def __init__(self, l2_reg=0.02):\n",
        "        super(NpairLoss, self).__init__()\n",
        "        self.l2_reg = l2_reg\n",
        "\n",
        "    def forward(self, anchor, positive, target):\n",
        "        batch_size = anchor.size(0)\n",
        "        target = target.view(target.size(0), 1)\n",
        "\n",
        "        target = (target == torch.transpose(target, 0, 1)).float()\n",
        "        target = target / torch.sum(target, dim=1, keepdim=True).float()\n",
        "\n",
        "        logit = torch.matmul(anchor, torch.transpose(positive, 0, 1))\n",
        "        loss_ce = F.cross_entropy(logit, target)\n",
        "        l2_loss = torch.sum(anchor**2) / batch_size + torch.sum(positive**2) / batch_size\n",
        "\n",
        "        loss = loss_ce + self.l2_reg*l2_loss*0.25\n",
        "        return loss\n",
        "\n",
        "# Create an instance of the NpairLoss\n",
        "npair_loss = NpairLoss()\n",
        "\n",
        "# Instantiate the trainer without specifying compute_loss argument\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=dataset,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    dataset_num_proc=2,\n",
        "    packing=False,\n",
        "    args=TrainingArguments(\n",
        "        per_device_train_batch_size=2,\n",
        "        gradient_accumulation_steps=4,\n",
        "        warmup_steps=5,\n",
        "        max_steps=60,\n",
        "        learning_rate=2e-4,\n",
        "        fp16=not torch.cuda.is_bf16_supported(),\n",
        "        bf16=torch.cuda.is_bf16_supported(),\n",
        "        logging_steps=1,\n",
        "        optim=\"adamw_8bit\",\n",
        "        weight_decay=0.01,\n",
        "        lr_scheduler_type=\"linear\",\n",
        "        seed=3407,\n",
        "        output_dir=\"outputs\",\n",
        "    )\n",
        ")\n",
        "\n",
        "# Set the custom loss function to the model\n",
        "trainer.model.compute_loss = npair_loss\n",
        "trainer_stats = trainer.train()\n"
      ],
      "metadata": {
        "id": "CLg2pL3lJa1p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inference/Testing."
      ],
      "metadata": {
        "id": "kDpYDzFmJqwC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import PreTrainedTokenizerFast\n",
        "\n",
        "prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Output:\n",
        "\"\"\"\n",
        "\n",
        "texts=[]\n",
        "def formatting_prompts_func(examples):\n",
        "    instruction = \"You are an expert in identifying whether a statement contains sentiments against certain racial groups or ethnic communities. Given a statement, your task is to classify the given text to determine if it holds sentiments against racial groups or ethnic communities.\"\n",
        "    input = examples[\"text\"]\n",
        "    #print(\"input: \",input)\n",
        "    output = examples[\"label\"]\n",
        "    #print(\"output:\",output)\n",
        "\n",
        "    s=prompt.format(instruction, input)\n",
        "    #print(s)\n",
        "    texts.append(s)\n",
        "    return { \"text\" : s }\n",
        "\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "test_dataset=val_dataset.map(formatting_prompts_func)\n"
      ],
      "metadata": {
        "id": "H2VDqdelJrj1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "import time\n",
        "\n",
        "generated_outputs = []\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "# Iterate through the test dataset\n",
        "for idx in range(len(test_dataset['text'])):\n",
        "    # Generate input prompt\n",
        "    prompt_input = tokenizer(test_dataset['text'][idx],return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "    # Generate output\n",
        "    outputs = model.generate(**prompt_input, max_new_tokens = 64, use_cache = True).to(\"cuda\")\n",
        "\n",
        "    print(tokenizer.batch_decode(outputs))\n",
        "    # Append generated output to the list\n",
        "    generated_outputs.append(tokenizer.batch_decode(outputs))"
      ],
      "metadata": {
        "id": "MC_8H1NsJwxg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generated_labels = [item[0].split(\"### Output:\\n\")[1].strip().split(\" \")[0] for item in generated_outputs]\n",
        "\n",
        "print(list(set(generated_labels)))"
      ],
      "metadata": {
        "id": "BYkBQ1t8Jy_K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluating performance"
      ],
      "metadata": {
        "id": "RfmgBs1gJ4iq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Convert generated_outputs to binary labels (0 or 1)\n",
        "predicted_labels = [1 if label == \"hate\" else 0 for label in generated_outputs]\n",
        "\n",
        "# Convert y_val to binary labels (0 or 1)\n",
        "actual_labels = [1 if label == \"hate\" else 0 for label in y_val]\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(actual_labels, predicted_labels)\n",
        "\n",
        "# Calculate precision\n",
        "precision = precision_score(actual_labels, predicted_labels)\n",
        "\n",
        "# Calculate recall\n",
        "recall = recall_score(actual_labels, predicted_labels)\n",
        "\n",
        "# Calculate F1 score\n",
        "f1 = f1_score(actual_labels, predicted_labels)\n",
        "\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 Score:\", f1)"
      ],
      "metadata": {
        "id": "kNwThmNqJ5Gl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save current model"
      ],
      "metadata": {
        "id": "YXW2pCLmKAN2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"supremacist\")\n",
        "tokenizer.save_pretrained(\"supremacist\")"
      ],
      "metadata": {
        "id": "c1UQp5-dJ_lW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "# Zip the folder containing model and tokenizer files\n",
        "shutil.make_archive(\"supremacist\", \"zip\", \"supremacist\")"
      ],
      "metadata": {
        "id": "CO2U-Kt4KH8i"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}